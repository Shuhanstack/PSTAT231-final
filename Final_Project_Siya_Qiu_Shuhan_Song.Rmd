---
title: "Final_project"
author: "Siya Qiu (PSTAT 231) & Shuhan Song (5609417, PSTAT 231)"
date: "12/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "hide")
```

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
library(maps)
library(janitor)
library(plotly)
library(cluster)
library(flexclust)
library(tree)
library(maptree)
library(ISLR)
library(ROCR)
library(glmnet)
library(randomForest)
library(gbm)
```




##### 1. What makes voter behavior prediction (and thus election forecasting) a hard problem?

There are many factors that make voter behavior hard to predict. People can be easily influenced by unpredictable various events, like debates and superstorms, that will change their decision made during the polls. Even though polls are taken from individuals, supporters for one candidate have greater tendency to get invloved in polling especially under the influence from social media. Also, people feel shamed about voting for one candidate might lie during polling but actually vote for the ideal candidate during voting. Pollsters want the result to be random and will factor in their own corrections. But the corrections they made also vary. 

##### 2. What was unique to Nate Silver’s approach in 2012 that allowed him to achieve good predictions?

Nate correctly predicted each state's voting result in 2012 and added up to a correct nationwide prediction. The systematic errors in the prediction model accurately cancelled out each other. This allowed him to achieve good predictions.

##### 3. What went wrong in 2016? What do you think should be done to make future predictions better?

The systematic errors of multiple polls went wrong in the same direction in 2016 so that added up to a wrong prediction in 2016. Individual polls were wrong, aggregated to wrong state polls, and further led to a wrong forcast of the national election result. I think it is important for pollsters to take polls close to random sample and avoid making assumptions to factor in to their own corrections. Besides, I think actions to reduce lying or undecision during the pollings could make future predictions better. 

```{r, warning = FALSE, message = FALSE}
election.raw <- read_delim("data/election/election.csv", delim = ",") %>% 
  mutate(candidate = as.factor(candidate))

census_meta <- read_delim("data/census/metadata.csv", delim = ";", col_names = FALSE)

census <- read_delim("data/census/census.csv", delim = ",")
```

##### 4. Report the dimension of election.raw after removing rows with fips = 2000. Provide a reason for excluding them. Please make sure to use the same name election.raw before and after removing those observations. 
```{r}
kable(election.raw %>% filter(fips == 2000)) %>% 
  kable_styling(bootstrap_options = "striped")
```

- The dataset "election.raw" is comprised by data from three levels: country-level data, state-level data, and federal_level data. For country-level data, the dataset should show its FIPS (Federal Information Processing Standard) in the "fips" column, as well as the specific county name in the "county" column. However, for observations with "fips" equals to "2000", they do not have their county name specified in the "county" column. In order to protect the consistency of our data, we need to remove data with "fips" equal to "2000". <br> <br>After removal, the "election.raw" dataset now has `r dim(election.raw)[1]` observations and `r dim(election.raw)[2]` variables.
```{r}
election.raw <- election.raw %>% 
  filter(fips != 2000)

dim(election.raw)
```

##### 5. Remove summary rows from election.raw data 
```{r}
# seperate federal-level data
election_federal <- election.raw %>% 
  filter(state == "US")

# seperate state-level data 
election_state <- election.raw %>% 
  filter(is.na(county)) %>% 
  filter(fips != "US")

# seperate county-level data 
election <- election.raw %>% 
  filter(county != is.na(county))
```

##### 6. How many named presidential candidates were there in the 2016 election? Draw a bar chart of all votes received by each candidate. You can split this into multiple plots or may prefer to plot the results on a log scale. Either way, the results should be clear and legible!
```{r, message = FALSE, warning = FALSE}
# how many different candidates
candidate_table <- election %>% 
  select(candidate, votes) %>% 
  group_by(candidate) %>% 
  summarize_all(funs(sum)) %>% 
  mutate(log_total = log(votes))
```

- There are `r nrow(candidate_table)` categories of candidates in the 2016 election, in which `r (nrow(candidate_table)-1)` are refering to specific person and 1 category is for "None of these candidates".

```{r}
# plot the votes received by each candidate
ggplot(candidate_table, aes(x = fct_reorder(candidate, log_total), y = log_total))+
  geom_col(fill = "orange")+
  coord_flip()+ 
  theme_minimal()+
   labs(y = "votes on a log scale",
        x = "candidate name")
```

##### 7. Create variables county_winner and state_winner by taking the candidate with the highest proportion of votes. Hint: to create country_winner, start with election, group_by fips, compute total votes, and pct = votes/total. Then choose the highest row using top_n(variable state)winner is similar).
```{r}
county_winner <- election %>% 
  group_by(fips) %>%
  mutate(total = sum(votes)) %>% 
  mutate(pct = votes/total) %>% 
  top_n(1, pct)
  
state_winner <- election_state %>% 
  group_by(state) %>%
  mutate(total = sum(votes)) %>% 
  mutate(pct = votes/total) %>% 
  top_n(1, pct) 

```

##### 8. Draw county-level map by creating counties = map_data("county"). Color by county.
```{r}
countries <- map_data("county")
ggplot(data = countries)+
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white", size = 0.1)+
  coord_fixed(1.3)+
  guides(fill = F)
```


##### 9. Now color the map by the winning candidate for each state.  
```{r}
# create a common column by creating a new column for states named fips
# combine states variable and state_winner we created earlier using left_join().
states <- map_data("state")
fips = state.abb[match(states$region, tolower(state.name))]
states$state <- fips
states <- left_join(states, state_winner, by = "state") %>% 
  filter(candidate != is.na(candidate))

summary(states)
```

```{r}
ggplot(data = states)+
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), 
               color = "white",
               size = 0.3) +
  coord_fixed(1.3) +
  labs(x = "Longitude",
       y = "Latitude",
       fill = "Winner by state")
  
```

##### 10. The variable county does not have fips column. So we will create one by pooling information from maps::county.fips. Split the polyname column to region and subregion. Use left_join() combine county.fips into county. Also, left_join() previously created variable county_winner
```{r}
fips <- county.fips %>% 
  separate(polyname, c("region", "subregion"), ",")

county <- left_join(countries, fips, by = c("region","subregion")) %>% 
  mutate(fips = as.character(fips))

county <- left_join(county, county_winner, by = "fips") %>% 
  filter(candidate != is.na(candidate))
```

```{r}
ggplot(data = county)+
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size = 0.1)+
  coord_fixed(1.3) +
  labs(x = "Longitude",
       y = "Latitude",
       fill = "Winner by county")
```

##### 11. Create a visualization of your choice using census data. 

- The map shows the percentage of poverty in each state and how it may relate to the election result. As it shows in the map, states with higher percentage of poverty tend to vote for 	Donald Trump (filled as red), while states with lower percentage of poverty tend to vote for 	13	Hillary Clinton (filled as green)
```{r}
# calculate the number of poverty people in each state
census_poverty <- na.omit(census) %>% 
  mutate(num_poverty = (Poverty/100)*TotalPop) %>% 
  group_by(State) %>% 
  summarize(tot_poverty = sum(num_poverty),
            tot_state_pop = sum(TotalPop),
            pct_poverty = tot_poverty/tot_state_pop) 
```

```{r}
# left join state with census poverty
abb = state.abb[match(census_poverty$State, state.name)]
census_poverty$state <- abb
states <- left_join(states, census_poverty, by = "state")
```

```{r, message = FALSE, warning = FALSE}
# state location 
state_loc <- data.frame("State" = state.name,
                        "long" = state.center$x,
                        "lat" = state.center$y)

state_loc <- left_join(state_loc, census_poverty, by = "State") %>% 
  filter(state != "AK") %>% 
  filter(state != "HI")
```

```{r}
# poverty vs. election result 
ggplot() +
  geom_polygon(data = states, 
               aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size = 0.3) +
  geom_point(data = state_loc, 
             aes(x = long, y = lat, color = pct_poverty), 
             size = 5,
             alpha = 0.8)+
  scale_size_continuous(range = c(0.1,6))+
  scale_color_viridis_c(option = "viridis", begin = 1, end = 0)+
  coord_fixed(1.3) +
  labs(x = "Longitude",
       y = "Latitude",
       fill = "Winner by state",
       color = "Percentage of poverty")
```

##### 12. The census data contains high resolution information (more fine-grained than county-level). In this problem, we aggregate the information into county-level data by computing TotalPop-weighted average of each attributes for each county.
```{r}
# Clean census data census.del: start with census, filter out any rows with missing values, convert {Men, Employed, Citizen} attributes to percentages (meta data seems to be inaccurate), compute Minority attribute by combining {Hispanic, Black, Native, Asian, Pacific}, remove these variables after creating Minority, remove {Walk, PublicWork, Construction}. Many columns seem to be related, and, if a set that adds up to 100%, one column will be deleted.
census.del <- na.omit(census) %>% 
  mutate(Men = Men/TotalPop,
         Employed = Employed/TotalPop,
         Citizen = Citizen/TotalPop,
         minority = Hispanic+Black+Native+Asian+Pacific) %>% 
  select(-c(Hispanic,Black,Native,Asian,Pacific, Walk, PublicWork, Construction))
```

```{r}
# Sub-county census data, census.subct: start with census.del from above, group_by() two attributes {State, County}, use add_tally() to compute CountyTotal. Also, compute the weight by TotalPop/CountyTotal.
census.subct <- census.del %>% 
  group_by(State, County) %>% 
  add_tally(name = "CountyTotal", wt = TotalPop) %>% 
  mutate(weight = TotalPop/CountyTotal )

# multiply every data with the weight TotalPop/CountyTotal
census.ct <-  census.subct %>% 
  mutate_at(vars(c(Men:minority)), funs(.*weight)) %>% 
  select(-c(CountyTotal, weight)) %>%  
  group_by(State, County) %>% 
  mutate_at(vars(c(TotalPop:minority)), funs(sum))

census.ct <-  unique(census.ct)
census.ct <-  census.ct %>% 
  mutate_at(vars(c(Men, Citizen)), funs(.*100))
```

- Here is some example after information aggregating 

```{r, warning = FALSE, message = FALSE, results='hold'}
#County census data, census.ct: start with census.subct, use summarize_at() to compute weighted sum
example <- census.ct[1:5,]
example <- kable(example) %>% 
  kable_styling(bootstrap_options = "striped")
example
```

##### 13. Run PCA for both county & sub-county level data. Save the first two principle components PC1 and PC2 into a two-column data frame, call it ct.pc and subct.pc, respectively. Discuss whether you chose to center and scale the features before running PCA and the reasons for your choice. What are the three features with the largest absolute values of the first principal component? Which features have opposite signs and what does that mean about the correaltion between these features?
```{r, warning = FALSE, message = FALSE}
# first look at the variance of the variables
apply(census.del, 2, var)
```

- We checked the variables in the dataframe that their variances are vastly different from each other. If we do not center and scale the variables before performing PCA, then most of the PC that we observed would be driven by variables that have the largest variances. Thus, it is important to standardize the variables to have mean zero and sd of 1 beforming PCA [1].

```{r}
# pca for subcounty, select the first 2 PC
subct_data <- census.subct %>% 
  ungroup() %>% 
  select(-State, -County)
subct.pc <- prcomp(subct_data, center = TRUE, scale = TRUE)
subct.pc <- subct.pc$rotation
subct.pc <- data.frame(subct.pc) %>% 
  select(PC1, PC2)
```

```{r}
# pca for county, select the first 2 PC
ct_data <- census.ct %>% 
  ungroup() %>% 
  select(-State, -County)
ct.pc <- prcomp(ct_data, center = TRUE, scale = TRUE)
ct.pc <- ct.pc$rotation
ct.pc <- data.frame(ct.pc) %>% 
  select(PC1, PC2)
```

```{r}
# explore the largest 3 features for PC1 of subcounty
subct_3 <- subct.pc %>% 
  rownames_to_column("feature") %>% 
  arrange(-abs(PC1)) %>%
  head(3)
```

```{r}
# explore the largest 3 features for PC1 of county
ct_3 <- ct.pc %>% 
  rownames_to_column("feature") %>% 
  arrange(-abs(PC1)) %>%
  head(3)
```

```{r}
# look for negative features for subcounty
subct_ne = NULL
for(i in 1:3){
  if(subct_3[i,2] < 0){
    subct_ne <- append(subct_ne, subct_3[i,1])
  }
}
if(is.null(subct_ne)){
  subct_ne = "no feature is negative"
}
# look for negative features for county
ct_ne = NULL
for(i in 1:3){
  if(ct_3[i,2] < 0){
    ct_ne <- append(ct_ne, ct_3[i,1])
  }
}
if(is.null(ct_ne)){
  ct_ne = "no feature is negative"
}
```

- The three features with the largest absolute values of the first principal component from the sub-county level data are: `r subct_3[1,1]`, `r subct_3[2,1]`, and `r subct_3[3,1]`.

- The three features with the largest absolute values of the first principal component from the county level data are: `r ct_3[1,1]`, `r ct_3[2,1]`, and `r ct_3[3,1]`.

- The negative feature of the largest 3 subcounty feature is: `r subct_ne`.

- The negative feature of the largest 3 county feature is: `r ct_ne`.

- The negative sign means that `r subct_ne` affect the election result in an opposite direction comparing to other features

##### 14. Determine the number of minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. 
```{r}
# Plot proportion of variance explained (PVE) and cumulative PVE for both the county and sub-county analyses.
subct_pca = prcomp(subct_data, center = TRUE, scale = TRUE)
subct_var = subct_pca$sdev^2
subct_pve = subct_var/sum(subct_var)

ct_pca = prcomp(ct_data, center = TRUE, scale = TRUE)
ct_var = ct_pca$sdev^2
ct_pve = ct_var/sum(ct_var)
```

```{r}
subct_cumsum_pve <- cumsum(subct_pve)
subct_cumsum_list <- NULL
for(i in subct_cumsum_pve){
    if(i < 0.9){
    subct_cumsum_list <- append(subct_cumsum_list, i)
    }
}
subct_num_90 <- length(subct_cumsum_list)+1

ct_cumsum_pve <- cumsum(ct_pve)
ct_cumsum_list <- NULL
for(i in ct_cumsum_pve){
    if(i < 0.9){
    ct_cumsum_list <- append(ct_cumsum_list, i)
    }
}
ct_num_90 <- length(ct_cumsum_list)+1
```

- See the following two plots for PVE and cumulative PVE for sub-county level data.

- As shown in the second plot, the Cumulative Proportion of Variance Explained, we need at least `r subct_num_90` PCs to capture 90% of the variance in the sub-county level analyses.

```{r}
par(mfrow = c(1, 2))

plot(subct_pve, xlab="Principal Component for sub-county level", ylab="Proportion of Variance Explained", ylim = c(0,1), type = "b")

plot(cumsum(subct_pve), xlab="Principal Component for sub-county level", ylab="Cumulative Proportion of Variance Explained", ylim = c(0,1), type = "b")
abline(h = 0.9, col = "red")
abline(v = subct_num_90, lty = 3)
```

- See the following two plots for PVE and cumulative PVE for county-level data

- As shown in the second plot, the Cumulative Proportion of Variance Explained, we need at least `r ct_num_90` PCs to capture 90% of the variance in the county-level analyses.

```{r}
par(mfrow = c(1, 2))

plot(ct_pve, xlab="Principal Component for county level", ylab="Proportion of Variance Explained", ylim = c(0,1), type = "b")

plot(cumsum(ct_pve), xlab="Principal Component for county level", ylab="Cumulative Proportion of Variance Explained", ylim = c(0,1), type = "b")
abline(h = 0.9, col = "red")
abline(v = ct_num_90, lty = 3)
```



##### 15. With census.ct, perform hierarchical clustering with complete linkage. Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components of ct.pc as inputs instead of the originald features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate clusters? Comment on what you observe and discuss possible explanations for these observations.
```{r, message = FALSE, warning = FALSE}
# build the 10 clusters 
set.seed(123)
ct_dist <- dist(census.ct)
ct_hclust <- hclust(ct_dist, method = "complete")
ct_cluster_10 <- cutree(ct_hclust, 10)
```

```{r}
# find out the first 5 PC
ct_pc_5 <- prcomp(ct_data, center = TRUE, scale = TRUE)
ct_pc_5 <- ct_pc_5$x
ct_pc_5 <- data.frame(ct_pc_5) %>% 
  select(1:5)
ct_pc_5$State <- census.ct$State
ct_pc_5$County <- census.ct$County
```

```{r, warning = FALSE, message = FALSE}
# re-run using first 5 PC
set.seed(123)
ct_pc_5_dist <- dist(ct_pc_5)
ct_pc_5_hclust <- hclust(ct_pc_5_dist, method = "complete")
ct_5_cluster_10 <- cutree(ct_pc_5_hclust, 10)
```

- The two trees does not agree with each other. The 5 PC tree dissipate observations in the first cluster of the original tree to clusters 1 to 7.  

- See the following table for the contract result. Each column represents for the number of observation assigned to each cluster in the 5 PC tree. Each row represents for the number of observation assigned to each cluster in the original tree. 
```{r, warning = FALSE, message = FALSE, results = "hold"}
# compare resulte
contract_table <- table(org_cluster = ct_cluster_10, pc5_cluster = ct_5_cluster_10)
kable(contract_table, row.names = c(1,2,3,4,5,6,7,8,9,10)) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r}
# explore San Mateo is contained in which cluster 
census.ct$cluster <- ct_cluster_10 
ct_pc_5$cluster <- ct_5_cluster_10 
smc_row <- which(census.ct$County == "San Mateo")
smc_org <- census.ct$cluster[smc_row]
smc_5pc <- ct_pc_5$cluster[smc_row]
```

```{r}
org_cluster <- census.ct %>% 
  filter(cluster == smc_org)

pc5_cluster <- ct_pc_5 %>% 
  filter(cluster == smc_5pc)
```

- The original tree contains San Mateo County in cluster `r smc_org`.

- The 5 PC tree contains San Mateo County in cluster `r smc_5pc`.

- The five PC approch seems to put San Mateo County in a more appropriate clusters. Firstly, if we look at the cumulative PVE plot, 5 PC together can explain more than 60% of the variance. Secondly, the PC approach removed coorelation between variables, which is a good supplement to clustering. Thirdly, the county winner of San Mateo County is Hillary Clinton, which is more consistant with other counties within the same cluster for the 5PCapproach. 

```{r}
#——————————————————————————————————————————————————————————————————————
# Classification
#______________________________________________________________________
```

```{r}
# combine county_winner and census.ct into election.cl
census.ct <- census.ct %>% 
  select(-cluster)

tmpwinner <- county_winner %>% ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus <- census.ct %>% ungroup %>% mutate_at(vars(State, County), tolower)

election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

## save meta information
election.meta <- election.cl %>% select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% select(-c(county, fips, state, votes, pct, total))

```

```{r}
# Using the following code, partition data into 80% training and 20% testing:
set.seed(10) 
n <- nrow(election.cl)
in.trn <- sample.int(n, 0.8*n) 
trn.cl <- election.cl[ in.trn,]
tst.cl <- election.cl[-in.trn,]
```

```{r}
trn.cl <- trn.cl %>%
  mutate(candidate = as.factor(ifelse(candidate == "Hillary Clinton", "Hillary Clinton", "Donald Trump")))

tst.cl <- tst.cl %>%
  mutate(candidate = as.factor(ifelse(candidate == "Hillary Clinton", "Hillary Clinton", "Donald Trump")))
```


```{r}
# Using the following code, define 10 cross-validation folds:
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(trn.cl), breaks=nfold, labels=FALSE))
```

```{r}
# Using the following error rate function:
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```

##### 16. Decision tree: train a decision tree by cv.tree(). Prune tree to minimize misclassification error. Be sure to use the folds from above for cross-validation. Visualize the trees before and after pruning. Save training and test errors to records variable. Intepret and discuss the results of the decision tree analysis.
```{r}
# fit model on training set
set.seed(666)
elec_tree <- tree(candidate~., data = trn.cl)
```

- Tree before pruning 

```{r}
draw.tree(elec_tree, nodeinfo = TRUE, cex = 0.3)
title("Classification Tree Built on Training Set (Before Pruning)")
```

- Tree after pruning

```{r}
# use cv.tree to prune the tree
set.seed(666)
cv_tree <- cv.tree(elec_tree, rand = folds, FUN = prune.misclass, K = nfold)
# Best size
best.cv <- min(cv_tree$size[cv_tree$dev==min(cv_tree$dev)])
# draw tree of the best size 
elec_prune <- prune.tree(elec_tree, best = best.cv)
draw.tree(elec_prune, nodeinfo = TRUE, cex = 0.4)
title("Classification Tree Built on Training Set (After Pruning)")
```
```{r}
# predict training value and testing value
pred_train_elec <- predict(elec_prune, trn.cl[-1], type = "class")
pred_test_elec <- predict(elec_prune, tst.cl[-1], type = "class")
```

```{r}
# calculate the training errors and testing errors
elec_train_err <- calc_error_rate(data.frame(pred_train_elec),trn.cl[1])
elec_test_err <- calc_error_rate(data.frame(pred_test_elec),tst.cl[1])
```

```{r}
# fill in records
records[1,1] <- elec_train_err
records[1,2] <- elec_test_err
```

- The training error of decision tree is `r records[1,1]`, while the testing error is `r records[1,2]`. The training error and testing error are close to each other, and both errors are relatively small. This means that our training model does not overfit the training data and will correctly predict the election results at a rate of `r (1-records[1,2])*100`%.

```{r, results = "hold"}
kable(records) %>% 
  kable_styling(bootstrap_options = "striped")
```

- Base on the NYT infographic, the voting behavior in the US depends on:<br>
    1. The ethnicity of the county. People tends to vote for a candidate that is closer to their ethnicity.<br>
    2. The education rate. Counties with lower education rates tend to vote for Clinton.<br>
    3. Location of the county. Counties in the Northeast or South tend to vote for Clinton.<br>
    4. Poverty. Counties with high poverty rate tend to vote for Clinton.<br>
    5. Population density and political stand. 
    
##### 17. Run a logistic regression to predict the winning candidate in each county. Save training and test errors to records variable. What are the significant variables? Are the consistent with what you saw in decision tree analysis? Interpret the meaning of a couple of the significant coefficients in terms of a unit change in the variables.
```{r, warning = FALSE, message = FALSE}
# build glm model
glm.fit = glm(candidate~.,data = trn.cl, family = binomial)

# predict training value and testing value
pred_glm_train = data.frame(predict(glm.fit, trn.cl[-1], type = "response"))
colnames(pred_glm_train) = "prob"
pred_glm_train <- pred_glm_train %>% 
  mutate(pred = as.factor(ifelse(prob <= 0.5, "Donald Trump", "Hillary Clinton")))

pred_glm_test = data.frame(predict(glm.fit, tst.cl[-1], type = "response"))
colnames(pred_glm_test) = "prob"
pred_glm_test <- pred_glm_test %>% 
  mutate(pred = as.factor(ifelse(prob <= 0.5, "Donald Trump", "Hillary Clinton")))
```

```{r}
# calculate the training errors and testing errors
glm_train_err <- calc_error_rate(data.frame(pred_glm_train$pred),trn.cl[1])
glm_test_err <- calc_error_rate(data.frame(pred_glm_test$pred),tst.cl[1])
```

```{r}
# fill in record
records[2,1] <- glm_train_err
records[2,2] <- glm_test_err
```

```{r, results = "hold"}
kable(records) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r}
summary_glm_fit <- summary(glm.fit)
coe_var <- summary_glm_fit$coefficients[,4]
name_var <- names(coe_var)
sig_var <- NULL
for(i in 1:28){
  if(coe_var[i]<0.05){
    sig_var <- append(sig_var, name_var[i])
  }
}
sig_var <- sig_var[-1]

est_employed <- summary_glm_fit$coefficients[23,1]
est_unemployed <- summary_glm_fit$coefficients[27,1]
est_citizen <- summary_glm_fit$coefficients[6,1]
```

- Variables that are significant are: `r sig_var` (*p* < 0.05). It is not quite consistent with what we have in decision tree but has overlapped ones. 
- Interpretation: For example, for variable of *Employed*, when other vairables are hold constant, a unit increase of employed will increase the score of voting for Clinton by `r round(est_employed, 2)`. For variable *Unemployed*, a unit increase of employed will increase the score of voting for Clinton by only `r round(est_unemployed, 2)`. Also, for a unit increase of variable *Citizen*, it will increase the score of voting for Clinton by only `r round(est_citizen, 2)`. 
    
##### 18. You may notice that you get a warning glm.fit: fitted probabilities numerically 0 or 1 occurred. As we discussed in class, this is an indication that we have perfect separation (some linear combination of variables perfectly predicts the winner). This is usually a sign that we are overfitting. One way to control overfitting in logistic regression is through regulation. Use the cv.glmnet function from the glmnet library to run K-fold cross validation and select the best regularization parameter for the logistic regression with LASSO penalty. Reminder: set alpha=1 to run LASSO regression, set lambda = c(1, 5, 10, 50) * 1e-4 in cv.glmnet() function to set pre-defined candidate values for the tuning parameter λ. This is because the default candidate values of λ in cv.glmnet() is relatively too large for our dataset thus we use pre-defined candidate values. What is the optimal value of λ in cross validation? What are the non-zero coefficients in the LASSO regression for the optimal value of λ? How do they compare to the unpenalized logistic regression? Save training and test errors to the records variable.
```{r}
# control overfitting in logistic regression is through regularization
x.train <-  model.matrix(candidate~., data = trn.cl)[,-1]
x.test <-  model.matrix(candidate~., data = tst.cl)[,-1]
Ytrain.cl <-  trn.cl$candidate
Ytrain.cl <- factor(Ytrain.cl, levels = c("Donald Trump", "Hillary Clinton"))
Ytest.cl <- tst.cl$candidate
Ytest.cl <- factor(Ytest.cl, levels = c("Donald Trump", "Hillary Clinton"))

regu_glm <- cv.glmnet(x.train, Ytrain.cl, lambda = c(1, 5, 10, 50) * 1e-4, alpha = 1, family = "binomial")
optlambda <-  regu_glm$lambda.min
ans_opt <- as.character(optlambda)

coe <-  predict(regu_glm, s = optlambda, type ="coefficient")[1:27,]
coe_names <- names(coe)
nzcoe <-  NULL
for(i in 2:27){
  if(coe[i]!=0){
    nzcoe <- append(nzcoe, coe_names[i])
  }
}
```

- The optimal value of lanbda in cross validation is `r ans_opt`.
- The non-zero coefficients in the LASSO regression for the optimal value of lambda are `r nzcoe`. 

```{r}
# make predictions 
regu_pred_train = predict(regu_glm, s=optlambda, newx = x.train, type = "response")
regu_pred_train = factor(ifelse(regu_pred_train>0.5, "Hillary Clinton", "Donald Trump"))
regu_pred_test = predict(regu_glm, s=optlambda, newx = x.test, type = "response")
regu_pred_test = factor(ifelse(regu_pred_test>0.5, "Hillary Clinton", "Donald Trump"))
regu_train_err = calc_error_rate(regu_pred_train, Ytrain.cl)
regu_test_err = calc_error_rate(regu_pred_test, Ytest.cl)
records[3,1] = regu_train_err
records[3,2] = regu_test_err
```

- Traning error and testing error of the LASSO regression 
```{r, results = "hold"}
kable(records) %>% 
  kable_styling(bootstrap_options = "striped")
```

- Comparing the test error and train error of logistic regression and lasso regression, there is not much difference in their performance. 

##### 19. Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data. 
```{r}
pred_test_elec <- predict(elec_prune, tst.cl[-1], type = "vector")
pred_glm_test <-  predict(glm.fit, tst.cl[-1], type = "response")
regu_pred_test <-  predict(regu_glm, s=optlambda, newx = x.test, type = "response")
```

```{r}
# List of predictions
preds_list <- list(pred_test_elec[, 2], pred_glm_test, regu_pred_test)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(Ytest.cl), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Logistic Regression", "LASSO Regression"),
       fill = 1:m)
```

- According to the ROC curve, regression models did better classification than Decision tree. Soft classifiers (logistic and LASSO) outperformed binary classifer. 

```{r}
#________________________________________________________
# Question 20
#________________________________________________________
```

##### 20. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. 

- Here, we want to use random forest and boosting to do classifications, and compare the two methods
```{r}
trn.cl$candidate = factor(trn.cl$candidate, levels = c("Donald Trump", "Hillary Clinton"))
tst.cl$candidate = factor(tst.cl$candidate, levels = c("Donald Trump", "Hillary Clinton"))
rf_elec = randomForest(candidate ~., data=trn.cl, mtry=4, importance=TRUE)
```

```{r}
# prediction
rf_pred <- predict(rf_elec, newdata = tst.cl)
#confusion matrix
rf.err <- table(pred = rf_pred, truth = tst.cl$candidate)
rf.err
```
```{r}
#test error
rf_test_err = 1-sum(diag(rf.err))/sum(rf.err)
```
```{r}
# view the importance of each variable
varImpPlot(rf_elec, sort = TRUE, cex = 0.6)
```
```{r}
# rf important variable
rf_imp_var <- data.frame(importance(rf_elec)) %>% 
  rownames_to_column("feature") 
rf_imp_var_mean <- rf_imp_var %>% 
  arrange(-MeanDecreaseAccuracy) %>% 
  head(1)
rf_imp_var_gini <- rf_imp_var %>% 
  arrange(-MeanDecreaseGini) %>% 
  head(1)
```

```{r}
# boosting imortant variable
boost_elec <- gbm(ifelse(candidate == "Hillary Clinton", 1,0)~., data = trn.cl, distribution = "bernoulli", n.tree = 500)
summary(boost_elec)
```

```{r}
# boosting prediction
pred_boost <- predict(boost_elec, newdata = tst.cl, n.trees = 500, type = "response")
# decide the cutoff point to convert my answer back to Trump/Clinton
# confusion matrix
pred_boost = factor(ifelse(pred_boost>0.5, "Hillary Clinton", "Donald Trump"))

boost.err <- table(pred = pred_boost, truth = tst.cl$candidate)
test.boost.err <- 1-sum(diag(boost.err))/sum(boost.err)
```
- According to our analysis of random forest, the `r rf_imp_var_mean` is by far the most important variable in terms of Model Accuracy and Gini index. However, according to boosting, the most important variable is "White" and then followed by "Transit"
- The testing error for randon forest is `r rf.err`, and the testing error for boosting is `r test.boost.err`, which random forest has a lower testing error 

##### Reference
[1] James et. al. - An Introduction to Statistical Learning with Applications in R, Eigth Edition. Available at: http://www-bcf.usc.edu/~gareth/ISL/
